# Configurations for CNN-based disease normalization.
# Defaults for paths, experimental parameters, etc.

# Config syntax:
# - case-sensitive section headers
# - case-insensitive parameter names
# - no in-line comments
# - use ${param} or ${section:param} to reuse other parameter values
# - the parameter ${rootpath} is preset to the project root path
# - after interpolating ${...}, values are interpreted as JSON fragments;
#   if that fails, some more Boolean names are tried (True/False, On/Off);
#   otherwise a string is returned unchanged

## copied from /home/lenz/disease-normalization/runs/summaries/20181007-224545.txt
[DEFAULT]
workers = 0

[general]
dataset = ncbi-disease
training_subset = train1
prediction_subset = dev1
nil_symbol = NIL
#dataset_cache = ${rootpath}/runs/dscache/{}.tgz
dataset_cache = False
startup_scripts = []

[logging]
format = %(asctime)s - %(message)s
level = INFO
summary_fn = ${rootpath}/runs/summaries/${timestamp}.txt
prediction_fn = ${rootpath}/runs/predictions/${timestamp}.tsv
detailed_fn = ${rootpath}/runs/detailed/${timestamp}.{}.tsv
trec_eval_fn = ${rootpath}/runs/trec_eva/${timestamp}.{}.tsv

[candidates]
generator = SGramCosine(.3, 100)
	PhraseVecFixedSet(100, "mean", "emb_stem")
	Hyperonym
	Abbreviation
	Composite
	SymbolReplacement
oracle = {"train": 3, "predict": 0}
suppress_ambiguous = True
workers = 0

[emb]
sample_size = 50
context_size = 500
embedding_dim = 50
embedding_voc = 10000
vectorizer_cache = True
tokenizer = whitespace
preprocess = none
embedding_fn = ${rootpath}/data/embeddings/wvec_200_win-30_chiu-et-al.kv
trainable = False

[emb_sub]
sample_size = ${emb:sample_size}
context_size = ${emb:context_size}
embedding_dim = ${emb:embedding_dim}
embedding_voc = ${emb:embedding_voc}
vectorizer_cache = ${emb:vectorizer_cache}
tokenizer = bpe
tokenizer_model = ${rootpath}/data/embeddings/bpe_pmcoa10000model
preprocess = none
embedding_fn = ${rootpath}/data/embeddings/bpe_pmcoa10000_vectors200.txt
trainable = False

[rank]
embeddings = ["emb"]
n_kernels = 50
filter_width = [3]
activation = tanh
optimizer = {"class_name": "adam", "config": {"amsgrad": true, "lr": 1e-4}}
loss = binary_crossentropy
epochs = 100
batch_size = 32
min_score = 0.0
workers = 3

[stop]
min_delta = 0
patience = 3
baseline = 0

[ncbi-disease]
corpus_dir = ${rootpath}/data/ncbi-disease/
dict_fn = ${rootpath}/data/ncbi-disease/MEDIC+UMLS.tsv

[share-clef]
corpus_dir = ${rootpath}/data/share-clef/
dict_fn = ${rootpath}/data/share-clef/SNOMED.tsv

[emb_stem]
sample_size = ${emb:sample_size}
context_size = ${emb:context_size}
embedding_dim = ${emb:embedding_dim}
embedding_voc = ${emb:embedding_voc}
vectorizer_cache = ${emb:vectorizer_cache}
tokenizer = whitespace
preprocess = stem
embedding_fn = ${rootpath}/data/embeddings/wvec_200_win-30_chiu-et-al.kv
trainable = False
