# Configurations for CNN-based disease normalization.
# Defaults for paths, experimental parameters, etc.

# Config syntax:
# - case-sensitive section headers
# - case-insensitive parameter names
# - no in-line comments
# - use ${param} or ${section:param} to reuse other parameter values
# - the parameter ${rootpath} is preset to the project root path
# - after interpolating ${...}, values are interpreted as JSON fragments;
#   if that fails, some more Boolean names are tried (True/False, On/Off);
#   otherwise a string is returned unchanged


[DEFAULT]

# defaults for all sections
## workers have to be 0!
workers = 0



[general]

# General options
# ===============

dataset = ncbi-disease
training_subset = train
prediction_subset = dev
nil_symbol = NIL

# Cache the entire preprocessing (candidate generation, vectorization etc.).
# Use a file name pattern containing "{}" to enable caching.
# Disable with a False value.
dataset_cache = ${rootpath}/runs/dscache/{}.tgz

# Startup scripts, eg. for configuring a Tensorflow session.
startup_scripts = []



[logging]

# Progress info and other diagnostic output
# =========================================

format = %(asctime)s - %(message)s
level = INFO

summary_fn = ${rootpath}/runs/summaries/${timestamp}.txt
prediction_fn = ${rootpath}/runs/predictions/${timestamp}.tsv
# detailed creates 3 output files (correct, reachable, unreachable)
detailed_fn = ${rootpath}/runs/detailed/${timestamp}.{}.tsv
# trec_eval creates 2 output files (prediction, gold)
trec_eval_fn = ${rootpath}/runs/trec_eva/${timestamp}.{}.tsv



[candidates]

# Candidate generation
# ====================

# To use multiple generators, add each on a separate line (indented)
generator = SGramCosine(.5, 10, [(2, 1), (3, 1)])
            PhraseVecFixedSet(10, "mean", "emb")
            ## insert RandomCandidates

# Oracle: add missing ground-truth names to the candidate set?
oracle = {"train": 0, "predict": 0}
random_sample = False



[emb]

# Word embeddings
# ===============

# tokens per mention
sample_size = 50
# tokens per paragraph
context_size = 500

# fallbacks if not using pretrained embeddings
embedding_dim = 50
embedding_voc = 10000

vectorizer_cache = True

# specific parameters (these are different for emb_sub):
# available: whitespace, charclass, bpe
tokenizer = whitespace
# available: stem, none
preprocess = none

embedding_fn = ${rootpath}/data/embeddings/wvec_50_haodi-li-et-al.bin
trainable = False



[emb_sub]

# Subword-unit embeddings
# =======================

# values copied from emb
sample_size = ${emb:sample_size}
context_size = ${emb:context_size}
embedding_dim = ${emb:embedding_dim}
embedding_voc = ${emb:embedding_voc}
vectorizer_cache = ${emb:vectorizer_cache}

# specific parameters
tokenizer = bpe
tokenizer_model = ${rootpath}/data/embeddings/bpe_abstract10000model
preprocess = none

embedding_fn = ${rootpath}/data/embeddings/bpe_vectors_10000_50_w2v.txt
trainable = False



[rank]

# CNN-based ranking
# =================

# embeddings: words ("emb") or subword-units ("emb_sub") or both?
embeddings = ["emb"]

# number of filters in the convolution
n_kernels = 50
filter_width = [3]
# used in convolution and hidden layer
activation = tanh

optimizer = {"class_name": "adadelta", "config": {"lr": 1.0}}
loss = binary_crossentropy
epochs = 10
batch_size = 32

# if the top-ranking score is below min_score, cast the ID to NIL
min_score = 0.0



[stop]

# Early-stopping callback for the CNN
# ===================================

# minimum change in the monitored quantity to qualify as an improvement
min_delta = 0
# number of epochs with no improvement after which training will be stopped
patience = 2
# training will stop if the model does not show improvement over the baseline
baseline = 0



# Dataset-specific parameters
# ===========================


[ncbi-disease]

corpus_dir = ${rootpath}/data/ncbi-disease/
dict_fn = ${rootpath}/data/ncbi-disease/CTD_diseases.tsv


[share-clef]

corpus_dir = ${rootpath}/data/share-clef/
dict_fn = ${rootpath}/data/share-clef/SNOMED.tsv
